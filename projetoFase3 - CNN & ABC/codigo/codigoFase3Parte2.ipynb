{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7fadadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-08 15:32:54.829587: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-08 15:32:55.326929: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-08 15:32:55.326964: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-08 15:32:56.672885: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-08 15:32:56.672952: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-08 15:32:56.672959: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import glob\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import random as python_random\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Activation, Conv2D, Dense, Dropout, Flatten, MaxPool2D, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import array_to_img, img_to_array, load_img, np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15515\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "\n",
    "IMAGE_WIDTH = 224    \n",
    "IMAGE_HEIGHT = 224\n",
    "image_size=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "\n",
    "train_paper = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/paper/*.jpg')\n",
    "train_cardboard = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/cardboard/*.jpg')\n",
    "train_plastic = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/plastic/*.jpg')\n",
    "train_metal = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/metal/*.jpg')\n",
    "train_trash = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/trash/*.jpg')\n",
    "train_battery = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/battery/*.jpg')\n",
    "train_shoes = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/shoes/*.jpg')\n",
    "train_clothes = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/clothes/*.jpg')\n",
    "train_glass = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/glass/*.jpg')\n",
    "train_biological = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/biological/*.jpg')\n",
    "\n",
    "train_files = [fn for fn in train_paper]+[fn for fn in train_cardboard]+[fn for fn in train_plastic]+[fn for fn in train_metal]+[fn for fn in train_trash]+[fn for fn in train_battery]+[fn for fn in train_shoes]+[fn for fn in train_clothes]+[fn for fn in train_glass]+[fn for fn in train_biological]\n",
    "print(len(train_files))\n",
    "\n",
    "train_imgs = [img_to_array(load_img(img, target_size=image_size)) for img in train_files]\n",
    "train_imgs = np.array(train_imgs)\n",
    "print(train_imgs.shape)\n",
    "\n",
    "train_labels = [0 for i in range(len(train_paper))]+[1 for i in range(len(train_cardboard))]+[2 for i in range(len(train_plastic))]+[3 for i in range(len(train_metal))]+[4 for i in range(len(train_trash))]+[5 for i in range(len(train_battery))]+[6 for i in range(len(train_shoes))]+[7 for i in range(len(train_clothes))]+[8 for i in range(len(train_glass))]+[9 for i in range(len(train_biological))]\n",
    "    \n",
    "val_paper = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/paper/*.jpg')\n",
    "val_cardboard = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/cardboard/*.jpg')\n",
    "val_plastic = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/plastic/*.jpg')\n",
    "val_metal = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/metal/*.jpg')\n",
    "val_trash = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/trash/*.jpg')\n",
    "val_battery = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/battery/*.jpg')\n",
    "val_shoes = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/shoes/*.jpg')\n",
    "val_clothes = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/clothes/*.jpg')\n",
    "val_glass = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/glass/*.jpg')\n",
    "val_biological = glob.glob('/home/manu/Documents/TP_IC/archive/garbage_classification/biological/*.jpg')\n",
    "\n",
    "val_files = [fn for fn in val_paper]+[fn for fn in val_cardboard]+[fn for fn in val_plastic]+[fn for fn in val_metal]+[fn for fn in val_trash]+[fn for fn in val_battery]+[fn for fn in val_shoes]+[fn for fn in val_clothes]+[fn for fn in val_glass]+[fn for fn in val_biological]\n",
    "val_imgs = [img_to_array(load_img(img, target_size=image_size)) for img in val_files]\n",
    "val_imgs = np.array(val_imgs)\n",
    "print(val_imgs.shape)\n",
    "val_labels= [0 for i in range(len(val_paper))]+[1 for i in range(len(val_cardboard))]+[2 for i in range(len(val_plastic))]+[3 for i in range(len(val_metal))]+[4 for i in range(len(val_trash))]+[5 for i in range(len(val_battery))]+[6 for i in range(len(val_shoes))]+[7 for i in range(len(val_clothes))]+[8 for i in range(len(val_glass))]+[9 for i in range(len(val_biological))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e996ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "epochs = 10\n",
    "IMAGE_CHANNELS = 3\n",
    "input_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "# encode text category labels\n",
    "train_labels_array = np.array(train_labels)\n",
    "le = LabelEncoder()\n",
    "train_integer_encoded = le.fit_transform(train_labels_array)\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "train_integer_encoded = train_integer_encoded.reshape(len(train_integer_encoded), 1)\n",
    "train_labels_ohe = ohe.fit_transform(train_integer_encoded)\n",
    "\n",
    "validation_labels_array = np.array(val_labels)\n",
    "validation_integer_encoded = le.fit_transform(validation_labels_array)\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "validation_integer_encoded = validation_integer_encoded.reshape(len(validation_integer_encoded), 1)\n",
    "validation_labels_ohe = ohe.fit_transform(validation_integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34628cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs_scaled = train_imgs.astype('float32')\n",
    "validation_imgs_scaled  = val_imgs.astype('float32')\n",
    "train_imgs_scaled /= 255\n",
    "validation_imgs_scaled /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab8173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(dense_layer_sizes, filters, kernel_size, pool_size):\n",
    "    '''Creates model comprised of 2 convolutional layers followed by dense layers\n",
    "\n",
    "    dense_layer_sizes: List of layer sizes. This list has one number for each layer\n",
    "    nb_filters: Number of convolutional filters in each convolutional layer\n",
    "    nb_conv: Convolutional kernel size\n",
    "    nb_pool: Size of pooling area for max pooling\n",
    "    '''\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters, (kernel_size, kernel_size),\n",
    "                     padding='valid', input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(filters, (kernel_size, kernel_size)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    for layer_size in dense_layer_sizes:\n",
    "        model.add(Dense(layer_size))\n",
    "        model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adadelta',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_size_candidates = [[32], [64], [32, 32], [64, 64]]\n",
    "my_classifier = KerasClassifier(make_model, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819657cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = GridSearchCV(my_classifier,\n",
    "                         param_grid={'dense_layer_sizes': dense_size_candidates,\n",
    "                                     # nb_epoch is avail for tuning even when not\n",
    "                                     # an argument to model building function\n",
    "                                     'epochs': [3, 6],\n",
    "                                     'filters': [8],\n",
    "                                     'kernel_size': [3],\n",
    "                                     'pool_size': [2]},\n",
    "                         scoring='neg_log_loss',\n",
    "                         n_jobs=1)\n",
    "validator.fit(train_imgs_scaled, train_labels_ohe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
